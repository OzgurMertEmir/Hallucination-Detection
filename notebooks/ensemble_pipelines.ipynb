{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "p9lYWbfwXJ9v",
        "XOh17kqiXVPe",
        "r5h0OoXNXrHn",
        "wCeveDyMX1nO",
        "bq3g7ErFYDex",
        "HqKBh_CoYKmv",
        "b5w3ylIGYYTP",
        "3jvA12gxYl9N",
        "diMneqSGYzr1"
      ],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup & config"
      ],
      "metadata": {
        "id": "p9lYWbfwXJ9v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Kul-ZFDTPZ_"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 1 – High-level setup & config\n",
        "# ============================================\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, Any, Tuple, List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    confusion_matrix,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "SEED = 42\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def set_global_seed(seed: int = 42) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_global_seed(SEED)\n",
        "\n",
        "DATASET_NAMES = [\n",
        "    \"defan\",\n",
        "    \"mmlu\",\n",
        "    \"halueval_qa\",\n",
        "    \"halueval_summarization\",\n",
        "    \"halueval_dialogue\",\n",
        "    \"psiloqa\",\n",
        "]\n",
        "\n",
        "PRETTY_DATASET_NAMES = {\n",
        "    \"defan\": \"Definitive Answers\",\n",
        "    \"mmlu\": \"MMLU-PRO\",\n",
        "    \"halueval_qa\": \"HaluEval QA\",\n",
        "    \"halueval_summarization\": \"HaluEval Summ.\",\n",
        "    \"halueval_dialogue\": \"HaluEval Dialogue\",\n",
        "    \"psiloqa\": \"PsiloQA\",\n",
        "    \"all_data\": \"All datasets\",\n",
        "}\n",
        "\n",
        "BASE_METHODS = [\n",
        "    \"factcheckmate\",\n",
        "    \"lap_eigvals\",\n",
        "    \"icr_probe\",\n",
        "    \"llm_check\",\n",
        "    \"attn_and_hiddn\",\n",
        "]\n",
        "\n",
        "MODELS_ROOT = \"models\"  # all models will be saved under models/<dataset>/<method>_model.pth\n",
        "\n",
        "os.makedirs(MODELS_ROOT, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Cell 2 – Colab / repo / feature extraction helpers\n",
        "# (optional – only needed if you want to regenerate feature_xxx.pkl)\n",
        "# ============================================\n",
        "import subprocess\n",
        "\n",
        "@dataclass\n",
        "class RepoConfig:\n",
        "    zip_path: str = \"halu_detector.zip\"\n",
        "    repo_dir: str = \"hallucination_detector\"\n",
        "    requirements_relpath: str = \"requirements.txt\"\n",
        "\n",
        "def setup_hallucination_repo(cfg: RepoConfig) -> None:\n",
        "    \"\"\"\n",
        "    Unzips the hallucination_detector repo and installs requirements.\n",
        "    Adapt paths to wherever your zip is stored.\n",
        "    \"\"\"\n",
        "    if os.path.exists(cfg.repo_dir):\n",
        "        print(f\"Removing existing {cfg.repo_dir}\")\n",
        "        subprocess.run([\"rm\", \"-rf\", cfg.repo_dir], check=False)\n",
        "\n",
        "    print(\"Unzipping repo...\")\n",
        "    subprocess.run([\"unzip\", \"-o\", cfg.zip_path], check=True)\n",
        "    # Clean macOS junk if present\n",
        "    subprocess.run([\"rm\", \"-rf\", \"__MACOSX\"], check=False)\n",
        "\n",
        "    # Install requirements\n",
        "    req_path = os.path.join(cfg.repo_dir, cfg.requirements_relpath)\n",
        "    print(\"Installing requirements from\", req_path)\n",
        "    subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", req_path],\n",
        "        check=True,\n",
        "    )\n",
        "    print(\"Repo setup complete.\")\n",
        "\n",
        "@dataclass\n",
        "class FeatureExtractionConfig:\n",
        "    repo_dir: str = \"hallucination_detector\"\n",
        "    model_name: str = \"Qwen/Qwen2.5-72B-Instruct\"\n",
        "    dataset_name: str = \"defan\"\n",
        "    output_dir: str = \"/content/drive/MyDrive/halu_features\"\n",
        "    max_examples: int = 2\n",
        "    batch_size: int = 1\n",
        "    max_new_tokens: int = 128\n",
        "    extra_args: Optional[List[str]] = None  # pass any extra CLI flags\n",
        "\n",
        "def run_feature_extraction(cfg: FeatureExtractionConfig) -> None:\n",
        "    \"\"\"\n",
        "    Thin wrapper around `python -m hallucination_detector.main ...`.\n",
        "    Assumes OPENAI_API_KEY (or other needed keys) are already in the env.\n",
        "    \"\"\"\n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        \"-m\",\n",
        "        \"hallucination_detector.main\",\n",
        "        \"--model_name\", cfg.model_name,\n",
        "        \"--dataset_name\", cfg.dataset_name,\n",
        "        \"--output_dir\", cfg.output_dir,\n",
        "        \"--max_examples\", str(cfg.max_examples),\n",
        "        \"--device\", DEVICE,\n",
        "        \"--batch_size\", str(cfg.batch_size),\n",
        "        \"--max_new_tokens\", str(cfg.max_new_tokens),\n",
        "    ]\n",
        "    if cfg.extra_args:\n",
        "        cmd.extend(cfg.extra_args)\n",
        "\n",
        "    print(\"Running feature extraction with command:\")\n",
        "    print(\" \".join(cmd))\n",
        "    subprocess.run(cmd, cwd=cfg.repo_dir, check=True)\n"
      ],
      "metadata": {
        "id": "NVEWSOyWTTJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "setup_hallucination_repo(RepoConfig())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "Gb2lI3c-T9N3",
        "outputId": "82d95031-9804-414f-debe-ab258900e2a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing existing hallucination_detector\n",
            "Unzipping repo...\n",
            "Installing requirements from hallucination_detector/requirements.txt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2124715793.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msetup_hallucination_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepoConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2861008655.py\u001b[0m in \u001b[0;36msetup_hallucination_repo\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mreq_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepo_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequirements_relpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Installing requirements from\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     subprocess.run(\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"install\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1199\u001b[0m                 \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   2051\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2052\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2053\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2054\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   2009\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2011\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2012\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Pickle"
      ],
      "metadata": {
        "id": "_UduNgtHXQb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Cell 3 – Load feature .pkl files & dataset utilities\n",
        "# ============================================\n",
        "@dataclass\n",
        "class FeaturePaths:\n",
        "    root: str = \"/content/drive/MyDrive/halu_features\"  # adjust to your Drive/path\n",
        "\n",
        "    def path(self, short_name: str) -> str:\n",
        "        return os.path.join(self.root, f\"features_{short_name}.pkl\")\n",
        "\n",
        "def _load_pickle(path: str):\n",
        "    with open(path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def load_raw_feature_pickles(paths: FeaturePaths) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Loads the raw CollatedMethodFeatures dicts for each original .pkl.\n",
        "    \"\"\"\n",
        "    print(\"Loading raw feature pickles from\", paths.root)\n",
        "    data = {\n",
        "        \"defan\": _load_pickle(paths.path(\"defan\")),\n",
        "        \"mmlu\": _load_pickle(paths.path(\"mmlu\")),\n",
        "        \"halueval_qa\": _load_pickle(paths.path(\"halueval_qa\")),\n",
        "        \"halueval_summarization\": _load_pickle(paths.path(\"halueval_summarization\")),\n",
        "        \"halueval_dialogue_psiloqa\": _load_pickle(\n",
        "            paths.path(\"halueval_dialogue_psiloqa\")\n",
        "        ),\n",
        "    }\n",
        "    return data\n",
        "\n",
        "def split_dialogue_psiloqa(dialogue_psiloqa: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Split the combined HaluEval-dialogue + PsiloQA CollatedMethodFeatures\n",
        "    bundle into separate dicts keyed by method name.\n",
        "    \"\"\"\n",
        "    ref_qids = (\n",
        "        dialogue_psiloqa[\"factcheckmate\"]\n",
        "        .meta[\"qid\"]\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    ds_names = ref_qids.str.split(\":\", n=1).str[0]  # \"halueval/dialogue\" or \"psiloqa\"\n",
        "    datasets = ds_names.unique()\n",
        "    print(\"Found dialogue sub-datasets:\", datasets)\n",
        "\n",
        "    idx_by_ds = {\n",
        "        ds: np.where(ds_names == ds)[0]\n",
        "        for ds in datasets\n",
        "    }\n",
        "\n",
        "    def slice_cmf(cmf, idx: np.ndarray):\n",
        "        # scalars\n",
        "        if cmf.scalars is not None and not cmf.scalars.empty:\n",
        "            scalars = cmf.scalars.iloc[idx].reset_index(drop=True)\n",
        "        else:\n",
        "            scalars = cmf.scalars  # keep as-is (likely empty DF)\n",
        "\n",
        "        # meta\n",
        "        meta = cmf.meta.iloc[idx].reset_index(drop=True)\n",
        "\n",
        "        # tensors\n",
        "        tensors = {k: v[idx] for k, v in cmf.tensors.items()}\n",
        "\n",
        "        cls = cmf.__class__\n",
        "        return cls(scalars=scalars, tensors=tensors, meta=meta)\n",
        "\n",
        "    per_dataset = {}\n",
        "    for ds, idx in idx_by_ds.items():\n",
        "        ds_methods = {}\n",
        "        for method_name, cmf in dialogue_psiloqa.items():\n",
        "            ds_methods[method_name] = slice_cmf(cmf, idx)\n",
        "        per_dataset[ds] = ds_methods\n",
        "\n",
        "    halueval_dialogue = per_dataset[\"halueval/dialogue\"]\n",
        "    psiloqa = per_dataset[\"psiloqa\"]\n",
        "\n",
        "    return halueval_dialogue, psiloqa\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def merge_feature_dicts(list_of_dicts: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Merge multiple {method_name -> CollatedMethodFeatures} dicts along rows.\n",
        "    This matches the behavior you had in your `merge_feature_dicts` cell.\n",
        "    \"\"\"\n",
        "    # 1) sanity: same method keys everywhere\n",
        "    method_names = list(list_of_dicts[0].keys())\n",
        "    for d in list_of_dicts[1:]:\n",
        "        assert set(d.keys()) == set(method_names), \"Method keys differ between datasets!\"\n",
        "\n",
        "    merged: Dict[str, Any] = {}\n",
        "\n",
        "    for m in method_names:\n",
        "        meta_list = []\n",
        "        scalars_list = []\n",
        "        tensor_lists: Dict[str, List[torch.Tensor]] = defaultdict(list)\n",
        "\n",
        "        for d in list_of_dicts:\n",
        "            cmf = d[m]\n",
        "\n",
        "            # meta\n",
        "            meta_list.append(cmf.meta)\n",
        "\n",
        "            # scalars\n",
        "            if cmf.scalars is not None and (\n",
        "                len(cmf.scalars.columns) > 0 or len(cmf.scalars) > 0\n",
        "            ):\n",
        "                scalars_list.append(cmf.scalars)\n",
        "\n",
        "            # tensors\n",
        "            for tname, tval in cmf.tensors.items():\n",
        "                tensor_lists[tname].append(tval)\n",
        "\n",
        "        # --- meta with union of columns ---\n",
        "        meta_cols = sorted(set().union(*(df.columns for df in meta_list)))\n",
        "        meta_normed = [df.reindex(columns=meta_cols) for df in meta_list]\n",
        "        meta_merged = pd.concat(meta_normed, ignore_index=True)\n",
        "\n",
        "        # --- scalars (possibly empty) ---\n",
        "        if scalars_list:\n",
        "            scalar_cols = sorted(set().union(*(df.columns for df in scalars_list)))\n",
        "            scalars_normed = [df.reindex(columns=scalar_cols) for df in scalars_list]\n",
        "            scalars_merged = pd.concat(scalars_normed, ignore_index=True)\n",
        "        else:\n",
        "            # keep empty DF shape from first dataset\n",
        "            scalars_merged = list_of_dicts[0][m].scalars\n",
        "\n",
        "        # --- tensors with torch.cat along dim=0 ---\n",
        "        tensors_merged = {}\n",
        "        if tensor_lists:\n",
        "            for tname, tlist in tensor_lists.items():\n",
        "                base_shape = tlist[0].shape[1:]\n",
        "                assert all(\n",
        "                    t.shape[1:] == base_shape for t in tlist\n",
        "                ), f\"Tensor {tname} shape mismatch for {m}\"\n",
        "                tensors_merged[tname] = torch.cat(tlist, dim=0)\n",
        "        else:\n",
        "            tensors_merged = list_of_dicts[0][m].tensors\n",
        "\n",
        "        cls = list_of_dicts[0][m].__class__\n",
        "        merged[m] = cls(\n",
        "            scalars=scalars_merged,\n",
        "            tensors=tensors_merged,\n",
        "            meta=meta_merged,\n",
        "        )\n",
        "\n",
        "    return merged\n",
        "\n",
        "# --- High-level data loader (caches in memory) ---\n",
        "_ALL_DATASETS: Dict[str, Dict[str, Any]] = {}\n",
        "_ALL_DATA_MERGED: Optional[Dict[str, Any]] = None\n",
        "_BALANCED_ALL_DATA_MERGED: Optional[Dict[str, Any]] = None\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from copy import deepcopy\n",
        "from typing import Dict, Literal\n",
        "\n",
        "# assuming CollatedMethodFeatures is already defined in your codebase:\n",
        "# from your_module import CollatedMethodFeatures\n",
        "\n",
        "def _subset_collated(cf, idx):\n",
        "    \"\"\"\n",
        "    Subset a single CollatedMethodFeatures by a numpy index array.\n",
        "    \"\"\"\n",
        "    # Subset scalars if they have rows\n",
        "    if cf.scalars is not None and len(cf.scalars) > 0:\n",
        "        scalars_sub = cf.scalars.iloc[idx].reset_index(drop=True)\n",
        "    else:\n",
        "        # Keep as-is (typically Empty DataFrame with shape (0, 0))\n",
        "        scalars_sub = cf.scalars\n",
        "\n",
        "    # Subset all tensors along the first dimension\n",
        "    tensors_sub = {}\n",
        "    for name, t in cf.tensors.items():\n",
        "        # assume first dimension is sample dimension\n",
        "        tensors_sub[name] = t[idx]\n",
        "\n",
        "    # Subset meta\n",
        "    meta_sub = cf.meta.iloc[idx].reset_index(drop=True)\n",
        "\n",
        "    return type(cf)(scalars=scalars_sub, tensors=tensors_sub, meta=meta_sub)\n",
        "\n",
        "\n",
        "def balance_all_data(\n",
        "    all_data: Dict[str, \"CollatedMethodFeatures\"],\n",
        "    strategy: Literal[\"downsample\", \"upsample\"] = \"downsample\",\n",
        "    seed: int = 42,\n",
        ") -> Dict[str, \"CollatedMethodFeatures\"]:\n",
        "    \"\"\"\n",
        "    Return a new all_data dict where class counts in 'label' are equal.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    all_data : dict\n",
        "        Mapping method_name -> CollatedMethodFeatures.\n",
        "        All methods are assumed to have the same number of samples and\n",
        "        aligned rows.\n",
        "    strategy : {\"downsample\", \"upsample\"}, default=\"downsample\"\n",
        "        - \"downsample\": reduce each class to the size of the smallest class.\n",
        "        - \"upsample\": increase each class to the size of the largest class\n",
        "          (with replacement).\n",
        "    seed : int, default=42\n",
        "        Random seed for reproducibility.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    balanced_all_data : dict\n",
        "        Same structure as all_data, but only (or additionally) containing\n",
        "        rows that yield equal class counts.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # Get labels from the first method's meta\n",
        "    first_key = next(iter(all_data))\n",
        "    labels = np.asarray(all_data[first_key].meta[\"label\"].values)\n",
        "\n",
        "    classes, counts = np.unique(labels, return_counts=True)\n",
        "\n",
        "    if strategy == \"downsample\":\n",
        "        target_n = counts.min()\n",
        "    elif strategy == \"upsample\":\n",
        "        target_n = counts.max()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
        "\n",
        "    # Build the new index set\n",
        "    new_indices = []\n",
        "    for cls, cnt in zip(classes, counts):\n",
        "        cls_indices = np.where(labels == cls)[0]\n",
        "\n",
        "        if strategy == \"downsample\":\n",
        "            chosen = rng.choice(cls_indices, size=target_n, replace=False)\n",
        "        else:  # upsample\n",
        "            replace = len(cls_indices) < target_n\n",
        "            chosen = rng.choice(cls_indices, size=target_n, replace=replace)\n",
        "\n",
        "        new_indices.append(chosen)\n",
        "\n",
        "    new_indices = np.concatenate(new_indices)\n",
        "    rng.shuffle(new_indices)\n",
        "\n",
        "    # Apply the same subset to every method\n",
        "    balanced_all_data = {}\n",
        "    for name, cf in all_data.items():\n",
        "        balanced_all_data[name] = _subset_collated(cf, new_indices)\n",
        "\n",
        "    return balanced_all_data\n",
        "\n",
        "def load_all_datasets(paths: FeaturePaths) -> Dict[str, Dict[str, Any]]:\n",
        "    global _ALL_DATASETS, _ALL_DATA_MERGED, _BALANCED_ALL_DATA_MERGED\n",
        "    if _ALL_DATASETS:\n",
        "        return _ALL_DATASETS\n",
        "\n",
        "    raw = load_raw_feature_pickles(paths)\n",
        "    halueval_dialogue, psiloqa = split_dialogue_psiloqa(\n",
        "        raw[\"halueval_dialogue_psiloqa\"]\n",
        "    )\n",
        "\n",
        "    datasets = {\n",
        "        \"defan\": raw[\"defan\"],\n",
        "        \"balanced_defan\": balance_all_data(raw[\"defan\"]),\n",
        "        \"mmlu\": raw[\"mmlu\"],\n",
        "        \"balanced_mmlu\": balance_all_data(raw[\"mmlu\"]),\n",
        "        \"halueval_qa\": raw[\"halueval_qa\"],\n",
        "        \"balanced_halueval_qa\": balance_all_data(raw[\"halueval_qa\"]),\n",
        "        \"halueval_summarization\": raw[\"halueval_summarization\"],\n",
        "        \"balanced_halueval_summarization\": balance_all_data(raw[\"halueval_summarization\"]),\n",
        "        \"halueval_dialogue\": halueval_dialogue,\n",
        "        \"balanced_halueval_dialogue\": balance_all_data(halueval_dialogue),\n",
        "        \"psiloqa\": psiloqa,\n",
        "        \"balanced_psiloqa\": balance_all_data(psiloqa),\n",
        "    }\n",
        "\n",
        "    _ALL_DATASETS = datasets\n",
        "    _ALL_DATA_MERGED = merge_feature_dicts([raw[\"defan\"], raw[\"mmlu\"],\n",
        "                                            raw[\"halueval_qa\"],\n",
        "                                            raw[\"halueval_summarization\"],\n",
        "                                            halueval_dialogue, psiloqa])\n",
        "    _BALANCED_ALL_DATA_MERGED = balance_all_data(_ALL_DATA_MERGED)\n",
        "    print(\"Datasets loaded. Keys:\", list(datasets.keys()))\n",
        "    return datasets\n",
        "\n",
        "def get_dataset(name: str, paths: FeaturePaths, isBalanced=False) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    name ∈ DATASET_NAMES or 'all_data'\n",
        "    \"\"\"\n",
        "    global _ALL_DATA_MERGED, _BALANCED_ALL_DATA_MERGED\n",
        "    datasets = load_all_datasets(paths)\n",
        "    if name == \"all_data\":\n",
        "        if _ALL_DATA_MERGED is None:\n",
        "            _ALL_DATA_MERGED = merge_feature_dicts(list(datasets.values()))\n",
        "        dataset = _ALL_DATA_MERGED\n",
        "    elif name == \"balanced_all_data\":\n",
        "        if _BALANCED_ALL_DATA_MERGED is None:\n",
        "            _BALANCED_ALL_DATA_MERGED = balance_all_data(_ALL_DATA_MERGED)\n",
        "        dataset = _BALANCED_ALL_DATA_MERGED\n",
        "    else:\n",
        "        dataset = datasets[name]\n",
        "\n",
        "    if isBalanced:\n",
        "        dataset = balance_all_data(dataset)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "XVk3-aUSTpOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy pandas\n",
        "!pip install --no-cache-dir numpy pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "collapsed": true,
        "id": "Wj8aFEIXTwin",
        "outputId": "47132725-4710-4ab5-89fd-f3f2cad185f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: pandas 2.3.3\n",
            "Uninstalling pandas-2.3.3:\n",
            "  Successfully uninstalled pandas-2.3.3\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m141.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas\n",
            "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m341.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m313.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m298.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, pandas\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "uqlm 0.4.5 requires numpy<2.0.0,>=1.26.4; python_version < \"3.13\", but you have numpy 2.3.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.3.5 pandas-2.3.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas"
                ]
              },
              "id": "9a8e5b5fb2834d1686caa79bee0add4a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = load_all_datasets(FeaturePaths())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YsHxFzs6UYJI",
        "outputId": "979f9073-3608-4836-d36a-13ab048a38c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading raw feature pickles from /content/drive/MyDrive/halu_features\n",
            "Found dialogue sub-datasets: ['halueval/dialogue' 'psiloqa']\n",
            "Datasets loaded. Keys: ['defan', 'balanced_defan', 'mmlu', 'balanced_mmlu', 'halueval_qa', 'balanced_halueval_qa', 'halueval_summarization', 'balanced_halueval_summarization', 'halueval_dialogue', 'balanced_halueval_dialogue', 'psiloqa', 'balanced_psiloqa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/halu_features/models/*.zip\" -d \"/content/models\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "2dSPTihB2P0B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b258fe73-0ac7-42f9-d5d3-86c7b59eb524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/halu_features/models/models_all_data.zip\n",
            "   creating: /content/models/models_all_data/\n",
            "  inflating: /content/models/models_all_data/attention_model.pth  \n",
            "  inflating: /content/models/models_all_data/lap_eigvals_model.pth  \n",
            "  inflating: /content/models/models_all_data/factcheckmate_model.pth  \n",
            "  inflating: /content/models/models_all_data/icr_probe_model.pth  \n",
            "  inflating: /content/models/models_all_data/llm_check_model.pth  \n",
            "   creating: /content/models/models_all_data/.ipynb_checkpoints/\n",
            "  inflating: /content/models/models_all_data/attn_and_hiddn_model.pth  \n",
            "\n",
            "Archive:  /content/drive/MyDrive/halu_features/models/models_defan.zip\n",
            "   creating: /content/models/models_defan/\n",
            "  inflating: /content/models/models_defan/attention_model.pth  \n",
            "  inflating: /content/models/models_defan/lap_eigvals_model.pth  \n",
            "  inflating: /content/models/models_defan/factcheckmate_model.pth  \n",
            "  inflating: /content/models/models_defan/icr_probe_model.pth  \n",
            "  inflating: /content/models/models_defan/llm_check_model.pth  \n",
            "   creating: /content/models/models_defan/.ipynb_checkpoints/\n",
            "  inflating: /content/models/models_defan/attn_and_hiddn_model.pth  \n",
            "\n",
            "Archive:  /content/drive/MyDrive/halu_features/models/models_mmlu.zip\n",
            "   creating: /content/models/models_mmlu/\n",
            "  inflating: /content/models/models_mmlu/attention_model.pth  \n",
            "  inflating: /content/models/models_mmlu/lap_eigvals_model.pth  \n",
            "  inflating: /content/models/models_mmlu/factcheckmate_model.pth  \n",
            "  inflating: /content/models/models_mmlu/config.json  \n",
            "  inflating: /content/models/models_mmlu/model.pth  \n",
            "  inflating: /content/models/models_mmlu/icr_probe_model.pth  \n",
            "  inflating: /content/models/models_mmlu/llm_check_model.pth  \n",
            "  inflating: /content/models/models_mmlu/attn_and_hiddn_model.pth  \n",
            "\n",
            "Archive:  /content/drive/MyDrive/halu_features/models/models_halueval_qa.zip\n",
            "   creating: /content/models/models_halueval_qa/\n",
            "  inflating: /content/models/models_halueval_qa/attention_model.pth  \n",
            "  inflating: /content/models/models_halueval_qa/lap_eigvals_model.pth  \n",
            "  inflating: /content/models/models_halueval_qa/factcheckmate_model.pth  \n",
            "  inflating: /content/models/models_halueval_qa/config.json  \n",
            "  inflating: /content/models/models_halueval_qa/model.pth  \n",
            "  inflating: /content/models/models_halueval_qa/icr_probe_model.pth  \n",
            "  inflating: /content/models/models_halueval_qa/llm_check_model.pth  \n",
            "  inflating: /content/models/models_halueval_qa/attn_and_hiddn_model.pth  \n",
            "\n",
            "Archive:  /content/drive/MyDrive/halu_features/models/models_halueval_summarization.zip\n",
            "   creating: /content/models/models_halueval_summarization/\n",
            "  inflating: /content/models/models_halueval_summarization/attention_model.pth  \n",
            "  inflating: /content/models/models_halueval_summarization/lap_eigvals_model.pth  \n",
            "  inflating: /content/models/models_halueval_summarization/factcheckmate_model.pth  \n",
            "  inflating: /content/models/models_halueval_summarization/icr_probe_model.pth  \n",
            "  inflating: /content/models/models_halueval_summarization/llm_check_model.pth  \n",
            "   creating: /content/models/models_halueval_summarization/.ipynb_checkpoints/\n",
            "  inflating: /content/models/models_halueval_summarization/attn_and_hiddn_model.pth  \n",
            "\n",
            "Archive:  /content/drive/MyDrive/halu_features/models/models_halueval_dialogue.zip\n",
            "   creating: /content/models/models_halueval_dialogue/\n",
            "  inflating: /content/models/models_halueval_dialogue/attention_model.pth  \n",
            "  inflating: /content/models/models_halueval_dialogue/lap_eigvals_model.pth  \n",
            "  inflating: /content/models/models_halueval_dialogue/factcheckmate_model.pth  \n",
            "  inflating: /content/models/models_halueval_dialogue/config.json  \n",
            "  inflating: /content/models/models_halueval_dialogue/model.pth  \n",
            "  inflating: /content/models/models_halueval_dialogue/icr_probe_model.pth  \n",
            "  inflating: /content/models/models_halueval_dialogue/llm_check_model.pth  \n",
            "  inflating: /content/models/models_halueval_dialogue/attn_and_hiddn_model.pth  \n",
            "\n",
            "Archive:  /content/drive/MyDrive/halu_features/models/models_psiloqa.zip\n",
            "   creating: /content/models/models_psiloqa/\n",
            "  inflating: /content/models/models_psiloqa/attention_model.pth  \n",
            "  inflating: /content/models/models_psiloqa/lap_eigvals_model.pth  \n",
            "  inflating: /content/models/models_psiloqa/factcheckmate_model.pth  \n",
            "  inflating: /content/models/models_psiloqa/config.json  \n",
            "  inflating: /content/models/models_psiloqa/model.pth  \n",
            "  inflating: /content/models/models_psiloqa/icr_probe_model.pth  \n",
            "  inflating: /content/models/models_psiloqa/llm_check_model.pth  \n",
            "  inflating: /content/models/models_psiloqa/attn_and_hiddn_model.pth  \n",
            "\n",
            "7 archives were successfully processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generic metric helpers"
      ],
      "metadata": {
        "id": "XOh17kqiXVPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Cell 4 – Generic metric helpers\n",
        "# ============================================\n",
        "def compute_binary_metrics(\n",
        "    y_true: np.ndarray,\n",
        "    y_prob: np.ndarray,\n",
        "    threshold: float = 0.5,\n",
        ") -> Dict[str, Any]:\n",
        "    y_true = np.asarray(y_true).astype(int)\n",
        "    y_prob = np.asarray(y_prob).astype(float)\n",
        "    y_pred = (y_prob >= threshold).astype(int)\n",
        "\n",
        "    # handle degenerate cases\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, y_prob)\n",
        "    except Exception:\n",
        "        auc = float(\"nan\")\n",
        "\n",
        "    try:\n",
        "        ap = average_precision_score(y_true, y_prob)\n",
        "    except Exception:\n",
        "        ap = float(\"nan\")\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    if cm.shape == (2, 2):\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "    else:\n",
        "        # if something weird happens, pad to 2x2\n",
        "        cm_full = np.zeros((2, 2), dtype=int)\n",
        "        cm_full[: cm.shape[0], : cm.shape[1]] = cm\n",
        "        tn, fp, fn, tp = cm_full.ravel()\n",
        "        cm = cm_full\n",
        "\n",
        "    return {\n",
        "        \"auroc\": auc,\n",
        "        \"ap\": ap,\n",
        "        \"acc\": acc,\n",
        "        \"f1\": f1,\n",
        "        \"precision\": prec,\n",
        "        \"recall\": rec,\n",
        "        \"threshold\": float(threshold),\n",
        "        \"cm\": cm,\n",
        "        \"tn\": int(tn),\n",
        "        \"fp\": int(fp),\n",
        "        \"fn\": int(fn),\n",
        "        \"tp\": int(tp),\n",
        "        \"pred_pos_rate\": float((fp + tp) / max(len(y_true), 1)),\n",
        "    }\n",
        "\n",
        "def sweep_thresholds(\n",
        "    y_true: np.ndarray,\n",
        "    y_prob: np.ndarray,\n",
        "    thresholds: Optional[np.ndarray] = None,\n",
        "    metric: str = \"acc\",\n",
        ") -> Dict[str, Any]:\n",
        "    if thresholds is None:\n",
        "        thresholds = np.linspace(0.2, 0.8, 13)\n",
        "\n",
        "    outs = []\n",
        "    for t in thresholds:\n",
        "        m = compute_binary_metrics(y_true, y_prob, threshold=float(t))\n",
        "        outs.append(m)\n",
        "\n",
        "    best = max(outs, key=lambda x: x.get(metric, float(\"-inf\")))\n",
        "    return best\n"
      ],
      "metadata": {
        "id": "GuBnolJeVvuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FactCheckmate"
      ],
      "metadata": {
        "id": "r5h0OoXNXrHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ============================================\n",
        "# Cell 5 – FactCheckmate model (MLP) – train & evaluate\n",
        "# ============================================\n",
        "@dataclass\n",
        "class FactCheckmateConfig:\n",
        "    hidden_dim: int = 256\n",
        "    dropout: float = 0.1\n",
        "    lr: float = 1e-4\n",
        "    batch_size: int = 256\n",
        "    max_epochs: int = 50\n",
        "    patience: int = 6\n",
        "    threshold_min: float = 0.2\n",
        "    threshold_max: float = 0.8\n",
        "    threshold_steps: int = 13\n",
        "    seed: int = SEED\n",
        "\n",
        "class FactCheckmateMLP(nn.Module):\n",
        "    def __init__(self, in_dim: int, hidden_dim: int = 256, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(-1)  # logits\n",
        "\n",
        "class NPDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        X = np.asarray(X, dtype=np.float32)\n",
        "        y = np.asarray(y, dtype=np.float32)\n",
        "        self.X = torch.from_numpy(X)\n",
        "        self.y = torch.from_numpy(y).float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "def _factcheckmate_xy(dataset: Dict[str, Any]) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    cmf = dataset[\"factcheckmate\"]\n",
        "    tensors = cmf.tensors[\"fcm_pooled_ffn\"]\n",
        "    labels = cmf.meta[\"label\"]\n",
        "    X_np = tensors.detach().cpu().numpy().astype(\"float32\")\n",
        "    y_np = labels.astype(int).to_numpy()\n",
        "    return X_np, y_np\n",
        "\n",
        "def _balanced_train_val_test_split(\n",
        "    X_np: np.ndarray,\n",
        "    y_np: np.ndarray,\n",
        "    seed: int = SEED,\n",
        ") -> Tuple[np.ndarray, ...]:\n",
        "    X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
        "        X_np,\n",
        "        y_np,\n",
        "        test_size=0.30,\n",
        "        random_state=seed,\n",
        "        stratify=y_np,\n",
        "    )\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_tmp,\n",
        "        y_tmp,\n",
        "        test_size=0.50,\n",
        "        random_state=seed,\n",
        "        stratify=y_tmp,\n",
        "    )\n",
        "\n",
        "    pos_idx = np.where(y_train == 1)[0]\n",
        "    neg_idx = np.where(y_train == 0)[0]\n",
        "    m = min(len(pos_idx), len(neg_idx))\n",
        "    rng = np.random.default_rng(seed)\n",
        "    pos_sel = rng.choice(pos_idx, size=m, replace=False)\n",
        "    neg_sel = rng.choice(neg_idx, size=m, replace=False)\n",
        "    sel = np.concatenate([pos_sel, neg_sel])\n",
        "    rng.shuffle(sel)\n",
        "\n",
        "    X_train_b = X_train[sel]\n",
        "    y_train_b = y_train[sel]\n",
        "\n",
        "    return X_train_b, y_train_b, X_val, y_val, X_test, y_test\n",
        "\n",
        "def _evaluate_factcheckmate_loader(\n",
        "    model: nn.Module,\n",
        "    loader: torch.utils.data.DataLoader,\n",
        "    threshold: float = 0.5,\n",
        ") -> Tuple[Dict[str, Any], np.ndarray]:\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    with torch.inference_mode():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(DEVICE)\n",
        "            logits = model(xb)\n",
        "            prob = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "            ys.append(yb.numpy())\n",
        "            ps.append(prob)\n",
        "\n",
        "    y_true = np.concatenate(ys).astype(int)\n",
        "    y_prob = np.concatenate(ps).astype(float)\n",
        "    metrics = compute_binary_metrics(y_true, y_prob, threshold=threshold)\n",
        "    return metrics, y_prob\n",
        "\n",
        "def _factcheckmate_model_path(dataset_name: str) -> str:\n",
        "    candidates = [\n",
        "        os.path.join(MODELS_ROOT, dataset_name, \"factcheckmate_model.pth\"),\n",
        "        f\"models_{dataset_name}/factcheckmate_model.pth\",          # legacy\n",
        "        f\"models/models_{dataset_name}/factcheckmate_model.pth\",   # legacy\n",
        "    ]\n",
        "    for p in candidates:\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    # default new location\n",
        "    path = os.path.join(MODELS_ROOT, dataset_name, \"factcheckmate_model.pth\")\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    return path\n",
        "\n",
        "def train_factcheckmate_for_dataset(\n",
        "    dataset: Dict[str, Any],\n",
        "    dataset_name: str,\n",
        "    cfg: FactCheckmateConfig,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Trains the FactCheckmate MLP on fcm_pooled_ffn for a single dataset.\n",
        "    Returns a dict with best state and train/val metrics.\n",
        "    \"\"\"\n",
        "    set_global_seed(cfg.seed)\n",
        "\n",
        "    X_np, y_np = _factcheckmate_xy(dataset)\n",
        "    print(f\"[{dataset_name}] FactCheckmate X shape: {X_np.shape}, y shape: {y_np.shape}\")\n",
        "    print(\"Positives:\", int(y_np.sum()), \"Negatives:\", int((1 - y_np).sum()))\n",
        "\n",
        "    X_train_b, y_train_b, X_val, y_val, X_test, y_test = _balanced_train_val_test_split(\n",
        "        X_np, y_np, seed=cfg.seed\n",
        "    )\n",
        "\n",
        "    train_ds = NPDataset(X_train_b, y_train_b)\n",
        "    val_ds = NPDataset(X_val, y_val)\n",
        "    test_ds = NPDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=2 * cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_ds,\n",
        "        batch_size=2 * cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    in_dim = X_np.shape[1]\n",
        "    model = FactCheckmateMLP(\n",
        "        in_dim=in_dim,\n",
        "        hidden_dim=cfg.hidden_dim,\n",
        "        dropout=cfg.dropout,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n",
        "\n",
        "    best_val_auc = -1\n",
        "    best_state = None\n",
        "    no_improve = 0\n",
        "\n",
        "    ts = np.linspace(cfg.threshold_min, cfg.threshold_max, cfg.threshold_steps)\n",
        "\n",
        "    for epoch in range(1, cfg.max_epochs + 1):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(DEVICE)\n",
        "            yb = yb.to(DEVICE).squeeze(-1)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item() * xb.size(0)\n",
        "        epoch_loss /= len(train_loader.dataset)\n",
        "\n",
        "        # evaluate at 0.5 and sweep thresholds on val\n",
        "        val_metrics_05, y_prob_val = _evaluate_factcheckmate_loader(\n",
        "            model, val_loader, threshold=0.5\n",
        "        )\n",
        "\n",
        "        # The following block was causing an error and is redundant.\n",
        "        # best_val = sweep_thresholds(\n",
        "        #     val_metrics_05[\"cm\"].sum(axis=1).repeat(0) if False else\n",
        "        #     np.concatenate([np.zeros_like(y_prob_val), np.ones_like(y_prob_val)]),\n",
        "        #     # ^ dummy so we don't re-use; we just want threshold grid\n",
        "        #     # but easier: just call sweep_thresholds on true labels directly:\n",
        "        #     None,\n",
        "        # )\n",
        "\n",
        "        # Actually use threshold sweep on the real val labels\n",
        "        val_labels = []\n",
        "        with torch.inference_mode():\n",
        "            for _, yb in val_loader:\n",
        "                val_labels.append(yb.numpy())\n",
        "        y_true_val = np.concatenate(val_labels).astype(int)\n",
        "        best_val = sweep_thresholds(y_true_val, y_prob_val, thresholds=ts, metric=\"acc\")\n",
        "        t_star = best_val[\"threshold\"]\n",
        "\n",
        "        val_metrics_star, _ = _evaluate_factcheckmate_loader(\n",
        "            model, val_loader, threshold=t_star\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"[{dataset_name}][Epoch {epoch:02d}] \"\n",
        "            f\"loss={epoch_loss:.4f} \"\n",
        "            f\"val_auc@0.5={val_metrics_05['auroc']:.4f} \"\n",
        "            f\"val_acc@t*={val_metrics_star['acc']:.4f} \"\n",
        "            f\"t*={t_star:.2f}\"\n",
        "        )\n",
        "\n",
        "        if val_metrics_star[\"auroc\"] > best_val_auc:\n",
        "            best_val_auc = val_metrics_star[\"auroc\"]\n",
        "            best_state = {\n",
        "                \"epoch\": epoch,\n",
        "                \"state_dict\": {k: v.cpu() for k, v in model.state_dict().items()},\n",
        "                \"threshold\": t_star,\n",
        "                \"in_dim\": in_dim,\n",
        "                \"hidden_dim\": cfg.hidden_dim,\n",
        "                \"dropout\": cfg.dropout,\n",
        "                \"lr\": cfg.lr,\n",
        "                \"seed\": cfg.seed,\n",
        "            }\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= cfg.patience:\n",
        "                print(\n",
        "                    f\"Early stopping at epoch {epoch}. \"\n",
        "                    f\"Best val auc={best_val_auc:.4f} \"\n",
        "                    f\"with t*={best_state['threshold']:.2f}\"\n",
        "                )\n",
        "                break\n",
        "\n",
        "    # Save best model\n",
        "    model_path = _factcheckmate_model_path(dataset_name)\n",
        "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
        "    torch.save(best_state, model_path)\n",
        "    print(\"Saved FactCheckmate model to\", model_path)\n",
        "\n",
        "    # Evaluate best model on test split\n",
        "    ckpt = best_state\n",
        "    best_model = FactCheckmateMLP(\n",
        "        in_dim=ckpt[\"in_dim\"],\n",
        "        hidden_dim=ckpt.get(\"hidden_dim\", 256),\n",
        "        dropout=ckpt.get(\"dropout\", 0.1),\n",
        "    ).to(DEVICE)\n",
        "    best_model.load_state_dict(ckpt[\"state_dict\"])\n",
        "    test_metrics, _ = _evaluate_factcheckmate_loader(\n",
        "        best_model, test_loader, threshold=ckpt[\"threshold\"]\n",
        "    )\n",
        "    print(\"Test metrics (FactCheckmate):\", test_metrics)\n",
        "\n",
        "    return {\n",
        "        \"dataset\": dataset_name,\n",
        "        \"config\": asdict(cfg),\n",
        "        \"best_state\": ckpt,\n",
        "        \"test_metrics\": test_metrics,\n",
        "    }\n",
        "\n",
        "def load_factcheckmate_model(dataset_name: str) -> Tuple[FactCheckmateMLP, Dict[str, Any]]:\n",
        "    path = _factcheckmate_model_path(dataset_name)\n",
        "    ckpt = torch.load(path, map_location=DEVICE)\n",
        "    model = FactCheckmateMLP(\n",
        "        in_dim=ckpt[\"in_dim\"],\n",
        "        hidden_dim=ckpt.get(\"hidden_dim\", 256),\n",
        "        dropout=ckpt.get(\"dropout\", 0.1),\n",
        "    ).to(DEVICE)\n",
        "    model.load_state_dict(ckpt[\"state_dict\"])\n",
        "    model.eval()\n",
        "    return model, ckpt\n",
        "\n",
        "def evaluate_factcheckmate_on_dataset(\n",
        "    model: FactCheckmateMLP,\n",
        "    dataset: Dict[str, Any],\n",
        "    batch_size: int = 256,\n",
        "    threshold: Optional[float] = None,\n",
        ") -> Dict[str, Any]:\n",
        "    X_np, y_np = _factcheckmate_xy(dataset)\n",
        "    ds = NPDataset(X_np, y_np)\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True\n",
        "    )\n",
        "    if threshold is None:\n",
        "        threshold = 0.5\n",
        "    metrics, y_prob = _evaluate_factcheckmate_loader(\n",
        "        model, loader, threshold=threshold\n",
        "    )\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "9lEhe1QrWMV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LapEigvals"
      ],
      "metadata": {
        "id": "wCeveDyMX1nO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Laplacian Eigenvalues ----------\n",
        "\n",
        "@dataclass\n",
        "class LapEigvalsConfig:\n",
        "    pca_components: int = 512\n",
        "    test_size: float = 0.3  # 60/20/20 via two splits\n",
        "    val_fraction_of_temp: float = 0.5\n",
        "    logreg_max_iter: int = 2000\n",
        "    logreg_C: float = 1.0\n",
        "    class_weight: str = \"balanced\"\n",
        "\n",
        "def _lap_xy(dataset: Dict[str, Any]) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    features = dataset[\"lap_eigvals\"].tensors[\"lap_eigvals_vector\"]\n",
        "    labels = dataset[\"lap_eigvals\"].meta[\"label\"]\n",
        "    X = features.detach().cpu().numpy()\n",
        "    y_series = labels\n",
        "    if not np.issubdtype(y_series.dtype, np.number):\n",
        "        y_series = pd.Categorical(y_series).codes\n",
        "    y = np.asarray(y_series, dtype=int).ravel()\n",
        "    return X, y\n",
        "\n",
        "def _lap_model_path(dataset_name: str) -> str:\n",
        "    path = os.path.join(MODELS_ROOT, dataset_name, \"lap_eigvals_model.pth\")\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    return path\n",
        "\n",
        "def train_lap_eigvals_for_dataset(\n",
        "    dataset: Dict[str, Any],\n",
        "    dataset_name: str,\n",
        "    cfg: LapEigvalsConfig,\n",
        ") -> Dict[str, Any]:\n",
        "    set_global_seed(SEED)\n",
        "    X, y = _lap_xy(dataset)\n",
        "    assert X.shape[0] == y.shape[0]\n",
        "    classes = np.unique(y)\n",
        "    assert len(classes) == 2, \"Expected binary labels for lap_eigvals.\"\n",
        "\n",
        "    X_tr, X_tmp, y_tr, y_tmp = train_test_split(\n",
        "        X, y, test_size=cfg.test_size, stratify=y, random_state=SEED\n",
        "    )\n",
        "    X_val, X_te, y_val, y_te = train_test_split(\n",
        "        X_tmp,\n",
        "        y_tmp,\n",
        "        test_size=cfg.val_fraction_of_temp,\n",
        "        stratify=y_tmp,\n",
        "        random_state=SEED,\n",
        "    )\n",
        "\n",
        "    pca = PCA(n_components=cfg.pca_components)\n",
        "    Xtr_p = pca.fit_transform(X_tr)\n",
        "    Xval_p = pca.transform(X_val)\n",
        "    Xte_p = pca.transform(X_te)\n",
        "\n",
        "    clf = LogisticRegression(\n",
        "        max_iter=cfg.logreg_max_iter,\n",
        "        C=cfg.logreg_C,\n",
        "        class_weight=cfg.class_weight,\n",
        "    )\n",
        "    clf.fit(Xtr_p, y_tr)\n",
        "\n",
        "    val_prob = clf.predict_proba(Xval_p)[:, 1]\n",
        "    te_prob = clf.predict_proba(Xte_p)[:, 1]\n",
        "\n",
        "    best_val = sweep_thresholds(y_val, val_prob, metric=\"acc\")\n",
        "    t_star = best_val[\"threshold\"]\n",
        "\n",
        "    val_metrics = compute_binary_metrics(y_val, val_prob, threshold=t_star)\n",
        "    test_metrics = compute_binary_metrics(y_te, te_prob, threshold=t_star)\n",
        "\n",
        "    print(f\"[{dataset_name}] LapEigvals – Val metrics:\", val_metrics)\n",
        "    print(f\"[{dataset_name}] LapEigvals – Test metrics:\", test_metrics)\n",
        "\n",
        "    state = {\n",
        "        \"pca\": pca,\n",
        "        \"clf\": clf,\n",
        "        \"threshold\": t_star,\n",
        "        \"config\": asdict(cfg),\n",
        "    }\n",
        "    torch.save(state, _lap_model_path(dataset_name))\n",
        "    print(\"Saved LapEigvals model to\", _lap_model_path(dataset_name))\n",
        "\n",
        "    return {\n",
        "        \"dataset\": dataset_name,\n",
        "        \"val_metrics\": val_metrics,\n",
        "        \"test_metrics\": test_metrics,\n",
        "        \"state\": state,\n",
        "    }\n",
        "\n",
        "def load_lap_model(dataset_name: str):\n",
        "    state = torch.load(_lap_model_path(dataset_name), map_location=\"cpu\", weights_only=False)\n",
        "    return state\n",
        "\n",
        "def evaluate_lap_eigvals_on_dataset(\n",
        "    state: Dict[str, Any],\n",
        "    dataset: Dict[str, Any],\n",
        ") -> Dict[str, Any]:\n",
        "    X, y = _lap_xy(dataset)\n",
        "    pca = state[\"pca\"]\n",
        "    clf = state[\"clf\"]\n",
        "    t_star = state[\"threshold\"]\n",
        "\n",
        "    X_p = pca.transform(X)\n",
        "    y_prob = clf.predict_proba(X_p)[:, 1]\n",
        "    metrics = compute_binary_metrics(y, y_prob, threshold=t_star)\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "YMbfRDt2X4aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ICR Probe"
      ],
      "metadata": {
        "id": "bq3g7ErFYDex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- ICR Probe (logistic over layer means) ----------\n",
        "\n",
        "@dataclass\n",
        "class ICRConfig:\n",
        "    logreg_max_iter: int = 2000\n",
        "    class_weight: str = \"balanced\"\n",
        "    test_size: float = 0.3\n",
        "    val_fraction_of_temp: float = 0.5\n",
        "\n",
        "def icr_layer_means(df: pd.DataFrame, scores_col: str = \"icr_scores\", label_col: str = \"label\"):\n",
        "    X_list = []\n",
        "    for cell in df[scores_col].tolist():\n",
        "        LxT = np.asarray(cell, dtype=np.float32)  # (36, T)\n",
        "        LxT = np.nan_to_num(LxT, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        feat = LxT.mean(axis=1).astype(np.float32)  # (36,)\n",
        "        X_list.append(feat)\n",
        "    X = np.stack(X_list, axis=0)\n",
        "    y = df[label_col].astype(int).to_numpy()\n",
        "    return X, y\n",
        "\n",
        "def _icr_model_path(dataset_name: str) -> str:\n",
        "    path = os.path.join(MODELS_ROOT, dataset_name, \"icr_probe_model.pth\")\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    return path\n",
        "\n",
        "def train_icr_probe_for_dataset(\n",
        "    dataset: Dict[str, Any],\n",
        "    dataset_name: str,\n",
        "    cfg: ICRConfig,\n",
        ") -> Dict[str, Any]:\n",
        "    cmf = dataset[\"icr_probe\"]\n",
        "    X, y = icr_layer_means(cmf.meta)\n",
        "\n",
        "    X_tr, X_tmp, y_tr, y_tmp = train_test_split(\n",
        "        X, y, test_size=cfg.test_size, stratify=y, random_state=SEED\n",
        "    )\n",
        "    X_val, X_te, y_val, y_te = train_test_split(\n",
        "        X_tmp,\n",
        "        y_tmp,\n",
        "        test_size=cfg.val_fraction_of_temp,\n",
        "        stratify=y_tmp,\n",
        "        random_state=SEED,\n",
        "    )\n",
        "\n",
        "    mean = X_tr.mean(axis=0, keepdims=True)\n",
        "    std = X_tr.std(axis=0, keepdims=True)\n",
        "    std[std < 1e-6] = 1.0\n",
        "    Xtr_n = (X_tr - mean) / std\n",
        "    Xval_n = (X_val - mean) / std\n",
        "    Xte_n = (X_te - mean) / std\n",
        "\n",
        "    clf = LogisticRegression(\n",
        "        max_iter=cfg.logreg_max_iter,\n",
        "        class_weight=cfg.class_weight,\n",
        "    )\n",
        "    clf.fit(Xtr_n, y_tr)\n",
        "\n",
        "    val_prob = clf.predict_proba(Xval_n)[:, 1]\n",
        "    te_prob = clf.predict_proba(Xte_n)[:, 1]\n",
        "\n",
        "    best_val = sweep_thresholds(y_val, val_prob, metric=\"acc\")\n",
        "    t_star = best_val[\"threshold\"]\n",
        "\n",
        "    val_metrics = compute_binary_metrics(y_val, val_prob, threshold=t_star)\n",
        "    test_metrics = compute_binary_metrics(y_te, te_prob, threshold=t_star)\n",
        "\n",
        "    state = {\n",
        "        \"mean\": mean,\n",
        "        \"std\": std,\n",
        "        \"clf\": clf,\n",
        "        \"threshold\": t_star,\n",
        "        \"config\": asdict(cfg),\n",
        "    }\n",
        "    torch.save(state, _icr_model_path(dataset_name))\n",
        "    print(\"Saved ICR model to\", _icr_model_path(dataset_name))\n",
        "\n",
        "    return {\n",
        "        \"dataset\": dataset_name,\n",
        "        \"val_metrics\": val_metrics,\n",
        "        \"test_metrics\": test_metrics,\n",
        "        \"state\": state,\n",
        "    }\n",
        "\n",
        "def load_icr_model(dataset_name: str):\n",
        "    return torch.load(_icr_model_path(dataset_name), map_location=\"cpu\", weights_only=False)\n",
        "\n",
        "def evaluate_icr_on_dataset(\n",
        "    state: Dict[str, Any],\n",
        "    dataset: Dict[str, Any],\n",
        ") -> Dict[str, Any]:\n",
        "    cmf = dataset[\"icr_probe\"]\n",
        "    X, y = icr_layer_means(cmf.meta)\n",
        "    mean, std = state[\"mean\"], state[\"std\"]\n",
        "    clf = state[\"clf\"]\n",
        "    t_star = state[\"threshold\"]\n",
        "    Xn = (X - mean) / std\n",
        "    y_prob = clf.predict_proba(Xn)[:, 1]\n",
        "    return compute_binary_metrics(y, y_prob, threshold=t_star)\n"
      ],
      "metadata": {
        "id": "8CXEXrpYYD88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM-Check"
      ],
      "metadata": {
        "id": "HqKBh_CoYKmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- LLM‑Check scalars (logistic pipeline) ----------\n",
        "\n",
        "@dataclass\n",
        "class LLMCheckConfig:\n",
        "    logreg_max_iter: int = 3000\n",
        "    C: float = 1.0\n",
        "    class_weight: str = \"balanced\"\n",
        "    test_size: float = 0.2\n",
        "\n",
        "def _llm_scalar_df(dataset: Dict[str, Any]) -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    llmc = dataset[\"llm_check\"].scalars\n",
        "    icr = dataset[\"icr_probe\"].scalars\n",
        "    features = pd.concat([llmc, icr], axis=1)\n",
        "    labels = dataset[\"llm_check\"].meta[\"label\"]\n",
        "    features = features.replace([np.inf, -np.inf], np.nan)\n",
        "    features = features.fillna(features.mean())\n",
        "    return features, labels\n",
        "\n",
        "def _llm_model_path(dataset_name: str) -> str:\n",
        "    path = os.path.join(MODELS_ROOT, dataset_name, \"llm_check_model.pth\")\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    return path\n",
        "\n",
        "def train_llmcheck_for_dataset(\n",
        "    dataset: Dict[str, Any],\n",
        "    dataset_name: str,\n",
        "    cfg: LLMCheckConfig,\n",
        ") -> Dict[str, Any]:\n",
        "    features, labels = _llm_scalar_df(dataset)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        features,\n",
        "        labels,\n",
        "        test_size=cfg.test_size,\n",
        "        stratify=labels,\n",
        "        random_state=SEED,\n",
        "    )\n",
        "\n",
        "    pipeline = make_pipeline(\n",
        "        StandardScaler(),\n",
        "        LogisticRegression(\n",
        "            class_weight=cfg.class_weight,\n",
        "            max_iter=cfg.logreg_max_iter,\n",
        "            C=cfg.C,\n",
        "        ),\n",
        "    )\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
        "    best = sweep_thresholds(y_test.to_numpy(), y_prob, metric=\"acc\")\n",
        "    t_star = best[\"threshold\"]\n",
        "    test_metrics = compute_binary_metrics(\n",
        "        y_test.to_numpy(), y_prob, threshold=t_star\n",
        "    )\n",
        "\n",
        "    state = {\n",
        "        \"pipeline\": pipeline,\n",
        "        \"threshold\": t_star,\n",
        "        \"config\": asdict(cfg),\n",
        "    }\n",
        "    torch.save(state, _llm_model_path(dataset_name))\n",
        "    print(\"Saved LLM‑Check model to\", _llm_model_path(dataset_name))\n",
        "\n",
        "    return {\n",
        "        \"dataset\": dataset_name,\n",
        "        \"test_metrics\": test_metrics,\n",
        "        \"state\": state,\n",
        "    }\n",
        "\n",
        "def load_llmcheck_model(dataset_name: str):\n",
        "    return torch.load(_llm_model_path(dataset_name), map_location=\"cpu\", weights_only=False)\n",
        "\n",
        "def evaluate_llmcheck_on_dataset(\n",
        "    state: Dict[str, Any],\n",
        "    dataset: Dict[str, Any],\n",
        ") -> Dict[str, Any]:\n",
        "    features, labels = _llm_scalar_df(dataset)\n",
        "    pipeline = state[\"pipeline\"]\n",
        "    t_star = state[\"threshold\"]\n",
        "    y_prob = pipeline.predict_proba(features)[:, 1]\n",
        "    return compute_binary_metrics(labels.to_numpy(), y_prob, threshold=t_star)\n"
      ],
      "metadata": {
        "id": "eV52vuKHYNJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention + Hidden"
      ],
      "metadata": {
        "id": "b5w3ylIGYYTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Attention + Hidden scores ----------\n",
        "\n",
        "@dataclass\n",
        "class AttnHiddenConfig:\n",
        "    logreg_max_iter: int = 3000\n",
        "    C: float = 1.0\n",
        "    class_weight: str = \"balanced\"\n",
        "    test_size: float = 0.2\n",
        "\n",
        "def _attn_hidden_features(dataset: Dict[str, Any]) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    attn_scores = dataset[\"llm_check\"].tensors[\"llmcheck_attn_scores\"]\n",
        "    hiddn_scores = dataset[\"llm_check\"].tensors[\"llmcheck_hidden_scores\"]\n",
        "    labels = dataset[\"llm_check\"].meta[\"label\"].astype(int).to_numpy()\n",
        "\n",
        "    attn_scores = np.nan_to_num(attn_scores, nan=np.nan, posinf=np.nan, neginf=np.nan)\n",
        "    attn_scores = np.nan_to_num(attn_scores, nan=np.nanmean(attn_scores, axis=0))\n",
        "\n",
        "    hiddn_scores = np.nan_to_num(hiddn_scores, nan=np.nan, posinf=np.nan, neginf=np.nan)\n",
        "    hiddn_scores = np.nan_to_num(hiddn_scores, nan=np.nanmean(hiddn_scores, axis=0))\n",
        "\n",
        "    combined = np.concatenate([attn_scores, hiddn_scores], axis=1)\n",
        "    return combined.astype(\"float32\"), labels\n",
        "\n",
        "def _attn_model_path(dataset_name: str) -> str:\n",
        "    path = os.path.join(MODELS_ROOT, dataset_name, \"attn_and_hiddn_model.pth\")\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    return path\n",
        "\n",
        "def train_attn_hidden_for_dataset(\n",
        "    dataset: Dict[str, Any],\n",
        "    dataset_name: str,\n",
        "    cfg: AttnHiddenConfig,\n",
        ") -> Dict[str, Any]:\n",
        "    X, y = _attn_hidden_features(dataset)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        test_size=cfg.test_size,\n",
        "        stratify=y,\n",
        "        random_state=SEED,\n",
        "    )\n",
        "\n",
        "    pipeline = make_pipeline(\n",
        "        StandardScaler(),\n",
        "        LogisticRegression(\n",
        "            class_weight=cfg.class_weight,\n",
        "            max_iter=cfg.logreg_max_iter,\n",
        "            C=cfg.C,\n",
        "        ),\n",
        "    )\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
        "    best = sweep_thresholds(y_test, y_prob, metric=\"acc\")\n",
        "    t_star = best[\"threshold\"]\n",
        "    test_metrics = compute_binary_metrics(y_test, y_prob, threshold=t_star)\n",
        "\n",
        "    state = {\n",
        "        \"pipeline\": pipeline,\n",
        "        \"threshold\": t_star,\n",
        "        \"config\": asdict(cfg),\n",
        "    }\n",
        "    torch.save(state, _attn_model_path(dataset_name))\n",
        "    print(\"Saved Attn+Hidden model to\", _attn_model_path(dataset_name))\n",
        "\n",
        "    return {\n",
        "        \"dataset\": dataset_name,\n",
        "        \"test_metrics\": test_metrics,\n",
        "        \"state\": state,\n",
        "    }\n",
        "\n",
        "def load_attn_hidden_model(dataset_name: str):\n",
        "    return torch.load(_attn_model_path(dataset_name), map_location=\"cpu\", weights_only=False)\n",
        "\n",
        "def evaluate_attn_hidden_on_dataset(\n",
        "    state: Dict[str, Any],\n",
        "    dataset: Dict[str, Any],\n",
        ") -> Dict[str, Any]:\n",
        "    X, y = _attn_hidden_features(dataset)\n",
        "    pipeline = state[\"pipeline\"]\n",
        "    t_star = state[\"threshold\"]\n",
        "    y_prob = pipeline.predict_proba(X)[:, 1]\n",
        "    return compute_binary_metrics(y, y_prob, threshold=t_star)"
      ],
      "metadata": {
        "id": "JQdrgllLYXhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Meta Detector"
      ],
      "metadata": {
        "id": "3jvA12gxYl9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Cell 7 – Meta‑ensemble over base method probabilities\n",
        "# ============================================\n",
        "@dataclass\n",
        "class MetaConfig:\n",
        "    # features: base probs + LLMCheck scalars\n",
        "    use_llm_scalars: bool = True\n",
        "    use_prob_summary: bool = True  # mean/max/min/std, num_over_08, num_over_06\n",
        "    logreg_C: float = 1.0\n",
        "    logreg_max_iter: int = 3000\n",
        "    gb_n_estimators: int = 300\n",
        "    gb_learning_rate: float = 0.05\n",
        "    gb_max_depth: int = 3\n",
        "    gb_subsample: float = 0.7\n",
        "\n",
        "def compute_base_probabilities_for_dataset(\n",
        "    dataset_name: str,\n",
        "    paths: FeaturePaths,\n",
        "    base_model_train_source: str = None,\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Compute base detector probabilities for a given dataset.\n",
        "    By default, assumes models were trained on the *same* dataset_name.\n",
        "    If you want cross‑dataset combinations, pass `base_model_train_source`\n",
        "    as the dataset that the base models were trained on.\n",
        "    \"\"\"\n",
        "    ds = get_dataset(dataset_name, paths)\n",
        "    train_src = base_model_train_source or dataset_name\n",
        "\n",
        "    # load base models trained on `train_src`\n",
        "    fcm_model, fcm_ckpt = load_factcheckmate_model(train_src)\n",
        "    lap_state = load_lap_model(train_src)\n",
        "    icr_state = load_icr_model(train_src)\n",
        "    llm_state = load_llmcheck_model(train_src)\n",
        "    attn_state = load_attn_hidden_model(train_src)\n",
        "\n",
        "    # FactCheckmate\n",
        "    X_fcm, y_fcm = _factcheckmate_xy(ds)\n",
        "    fcm_ds = NPDataset(X_fcm, y_fcm)\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        fcm_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True\n",
        "    )\n",
        "    fcm_model.eval()\n",
        "    fact_probs = []\n",
        "    ys_fcm = []\n",
        "    with torch.inference_mode():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(DEVICE)\n",
        "            logits = fcm_model(xb)\n",
        "            prob = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "            fact_probs.append(prob)\n",
        "            ys_fcm.append(yb.numpy())\n",
        "    fact_p = np.concatenate(fact_probs).astype(\"float32\")\n",
        "    y = np.concatenate(ys_fcm).astype(int)\n",
        "\n",
        "    # LapEigvals\n",
        "    lap_metrics = evaluate_lap_eigvals_on_dataset(lap_state, ds)\n",
        "    # we want probs – easiest is to recompute here:\n",
        "    X_lap, _ = _lap_xy(ds)\n",
        "    lap_p = lap_state[\"clf\"].predict_proba(lap_state[\"pca\"].transform(X_lap))[:, 1].astype(\n",
        "        \"float32\"\n",
        "    )\n",
        "\n",
        "    # ICR\n",
        "    X_icr, y_icr = icr_layer_means(ds[\"icr_probe\"].meta)\n",
        "    mean, std = icr_state[\"mean\"], icr_state[\"std\"]\n",
        "    Xn = (X_icr - mean) / std\n",
        "    icr_p = icr_state[\"clf\"].predict_proba(Xn)[:, 1].astype(\"float32\")\n",
        "\n",
        "    # LLMCheck\n",
        "    llm_features, _ = _llm_scalar_df(ds)\n",
        "    llm_p = llm_state[\"pipeline\"].predict_proba(llm_features)[:, 1].astype(\"float32\")\n",
        "\n",
        "    # Attn+Hidden\n",
        "    X_ah, _ = _attn_hidden_features(ds)\n",
        "    ah_p = attn_state[\"pipeline\"].predict_proba(X_ah)[:, 1].astype(\"float32\")\n",
        "\n",
        "    assert len(fact_p) == len(lap_p) == len(icr_p) == len(llm_p) == len(ah_p) == len(y)\n",
        "\n",
        "    return {\n",
        "        \"y\": y,\n",
        "        \"fact_p\": fact_p,\n",
        "        \"lap_p\": lap_p,\n",
        "        \"icr_p\": icr_p,\n",
        "        \"llmc_p\": llm_p,\n",
        "        \"attn_and_hiddn_p\": ah_p,\n",
        "    }\n",
        "\n",
        "def build_meta_dataframe(\n",
        "    dataset_name: str,\n",
        "    paths: FeaturePaths,\n",
        "    base_model_train_source: Optional[str] = None,\n",
        "    cfg: MetaConfig = MetaConfig(),\n",
        ") -> pd.DataFrame:\n",
        "    ds = get_dataset(dataset_name, paths)\n",
        "    base = compute_base_probabilities_for_dataset(\n",
        "        dataset_name, paths, base_model_train_source=base_model_train_source\n",
        "    )\n",
        "\n",
        "    meta_df = pd.DataFrame(\n",
        "        {\n",
        "            \"fact_p\": base[\"fact_p\"],\n",
        "            \"lap_p\": base[\"lap_p\"],\n",
        "            \"icr_p\": base[\"icr_p\"],\n",
        "            \"llmc_p\": base[\"llmc_p\"],\n",
        "            \"attn_and_hiddn_p\": base[\"attn_and_hiddn_p\"],\n",
        "            \"label\": base[\"y\"],\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if cfg.use_prob_summary:\n",
        "        feature_cols = [\"fact_p\", \"lap_p\", \"icr_p\", \"llmc_p\", \"attn_and_hiddn_p\"]\n",
        "        meta_df[\"mean_p\"] = meta_df[feature_cols].mean(axis=1)\n",
        "        meta_df[\"max_p\"] = meta_df[feature_cols].max(axis=1)\n",
        "        meta_df[\"min_p\"] = meta_df[feature_cols].min(axis=1)\n",
        "        meta_df[\"std_p\"] = meta_df[feature_cols].std(axis=1)\n",
        "        meta_df[\"num_over_08\"] = (\n",
        "            (meta_df[feature_cols] > 0.8).sum(axis=1).astype(\"int64\") / len(feature_cols)\n",
        "        )\n",
        "        meta_df[\"num_over_06\"] = (\n",
        "            (meta_df[feature_cols] > 0.6).sum(axis=1).astype(\"int64\") / len(feature_cols)\n",
        "        )\n",
        "\n",
        "    if cfg.use_llm_scalars:\n",
        "        llmc = ds[\"llm_check\"].scalars\n",
        "        icr = ds[\"icr_probe\"].scalars\n",
        "        features = pd.concat([llmc, icr], axis=1)\n",
        "        features = features.replace([np.inf, -np.inf], np.nan)\n",
        "        features = features.fillna(features.mean())\n",
        "        meta_df = pd.concat([meta_df, features.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    return meta_df\n",
        "\n",
        "def train_meta_logreg(\n",
        "    meta_df: pd.DataFrame,\n",
        "    cfg: MetaConfig,\n",
        ") -> Tuple[Any, Dict[str, Any]]:\n",
        "    y = meta_df[\"label\"].astype(int).to_numpy()\n",
        "    X = meta_df.drop(columns=[\"label\"])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        test_size=0.2,\n",
        "        stratify=y,\n",
        "        random_state=SEED,\n",
        "    )\n",
        "\n",
        "    pipeline = make_pipeline(\n",
        "        StandardScaler(),\n",
        "        LogisticRegression(\n",
        "            penalty=\"l2\",\n",
        "            C=cfg.logreg_C,\n",
        "            class_weight=\"balanced\",\n",
        "            max_iter=cfg.logreg_max_iter,\n",
        "            solver=\"lbfgs\",\n",
        "        ),\n",
        "    )\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
        "    best = sweep_thresholds(y_test, y_prob, metric=\"f1\")\n",
        "    t_star = best[\"threshold\"]\n",
        "    metrics = compute_binary_metrics(y_test, y_prob, threshold=t_star)\n",
        "\n",
        "    return (\n",
        "        {\"pipeline\": pipeline, \"threshold\": t_star, \"config\": asdict(cfg)},\n",
        "        metrics,\n",
        "    )\n",
        "\n",
        "def train_meta_gb(\n",
        "    meta_df: pd.DataFrame,\n",
        "    cfg: MetaConfig,\n",
        ") -> Tuple[GradientBoostingClassifier, Dict[str, Any]]:\n",
        "    y = meta_df[\"label\"].astype(int).to_numpy()\n",
        "    X = meta_df.drop(columns=[\"label\"]).to_numpy().astype(\"float32\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        test_size=0.2,\n",
        "        stratify=y,\n",
        "        random_state=SEED,\n",
        "    )\n",
        "\n",
        "    gb = GradientBoostingClassifier(\n",
        "        n_estimators=cfg.gb_n_estimators,\n",
        "        learning_rate=cfg.gb_learning_rate,\n",
        "        max_depth=cfg.gb_max_depth,\n",
        "        subsample=cfg.gb_subsample,\n",
        "        random_state=SEED,\n",
        "    )\n",
        "    gb.fit(X_train, y_train)\n",
        "\n",
        "    y_prob = gb.predict_proba(X_test)[:, 1]\n",
        "    best = sweep_thresholds(y_test, y_prob, metric=\"f1\")\n",
        "    t_star = best[\"threshold\"]\n",
        "    metrics = compute_binary_metrics(y_test, y_prob, threshold=t_star)\n",
        "    return gb, {\"threshold\": t_star, \"metrics\": metrics}\n"
      ],
      "metadata": {
        "id": "j1oqhWrqYfI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross‑dataset utilities\n"
      ],
      "metadata": {
        "id": "diMneqSGYzr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Cell 8 – Cross‑dataset utilities\n",
        "# ============================================\n",
        "def evaluate_base_method(\n",
        "    method: str,\n",
        "    train_dataset: str,\n",
        "    test_dataset: str,\n",
        "    paths: FeaturePaths,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Re‑use trained base models and evaluate on a *different* dataset.\n",
        "    \"\"\"\n",
        "    train_ds_name = train_dataset\n",
        "    test_ds = get_dataset(test_dataset, paths)\n",
        "\n",
        "    if method == \"factcheckmate\":\n",
        "        model, ckpt = load_factcheckmate_model(train_ds_name)\n",
        "        threshold = ckpt.get(\"threshold\", 0.5)\n",
        "        return evaluate_factcheckmate_on_dataset(model, test_ds, threshold=threshold)\n",
        "    elif method == \"lap_eigvals\":\n",
        "        state = load_lap_model(train_ds_name)\n",
        "        return evaluate_lap_eigvals_on_dataset(state, test_ds)\n",
        "    elif method == \"icr_probe\":\n",
        "        state = load_icr_model(train_ds_name)\n",
        "        return evaluate_icr_on_dataset(state, test_ds)\n",
        "    elif method == \"llm_check\":\n",
        "        state = load_llmcheck_model(train_ds_name)\n",
        "        return evaluate_llmcheck_on_dataset(state, test_ds)\n",
        "    elif method == \"attn_and_hiddn\":\n",
        "        state = load_attn_hidden_model(train_ds_name)\n",
        "        return evaluate_attn_hidden_on_dataset(state, test_ds)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown base method: {method}\")\n",
        "\n",
        "def run_cross_dataset_base_experiments(\n",
        "    paths: FeaturePaths,\n",
        "    datasets: List[str] = None,\n",
        "    methods: List[str] = None,\n",
        ") -> Dict[str, Dict[str, Dict[str, Dict[str, Any]]]]:\n",
        "    \"\"\"\n",
        "    Returns nested dict: results[method][train_ds][test_ds] -> metrics dict\n",
        "    \"\"\"\n",
        "    if datasets is None:\n",
        "        datasets = DATASET_NAMES\n",
        "    if methods is None:\n",
        "        methods = BASE_METHODS\n",
        "\n",
        "    results: Dict[str, Dict[str, Dict[str, Dict[str, Any]]]] = {}\n",
        "\n",
        "    for method in methods:\n",
        "        results[method] = {}\n",
        "        for train_ds in datasets:\n",
        "            results[method][train_ds] = {}\n",
        "            for test_ds in datasets:\n",
        "                if train_ds == test_ds:\n",
        "                    continue\n",
        "                print(f\"Evaluating {method}: train={train_ds}, test={test_ds}\")\n",
        "                metrics = evaluate_base_method(method, train_ds, test_ds, paths)\n",
        "                results[method][train_ds][test_ds] = metrics\n",
        "\n",
        "    return results\n",
        "\n",
        "def cross_results_to_dataframe(\n",
        "    results: Dict[str, Dict[str, Dict[str, Dict[str, Any]]]]\n",
        ") -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for method, train_dict in results.items():\n",
        "        for train_ds, test_dict in train_dict.items():\n",
        "            for test_ds, metrics in test_dict.items():\n",
        "                tn, fp, fn, tp = (\n",
        "                    metrics[\"tn\"],\n",
        "                    metrics[\"fp\"],\n",
        "                    metrics[\"fn\"],\n",
        "                    metrics[\"tp\"],\n",
        "                )\n",
        "                total = tn + fp + fn + tp\n",
        "                rows.append(\n",
        "                    {\n",
        "                        \"method\": method,\n",
        "                        \"train_ds\": train_ds,\n",
        "                        \"test_ds\": test_ds,\n",
        "                        \"auroc\": metrics[\"auroc\"],\n",
        "                        \"ap\": metrics[\"ap\"],\n",
        "                        \"acc\": metrics[\"acc\"],\n",
        "                        \"f1\": metrics[\"f1\"],\n",
        "                        \"precision\": metrics[\"precision\"],\n",
        "                        \"recall\": metrics[\"recall\"],\n",
        "                        \"threshold\": metrics[\"threshold\"],\n",
        "                        \"tn\": tn,\n",
        "                        \"fp\": fp,\n",
        "                        \"fn\": fn,\n",
        "                        \"tp\": tp,\n",
        "                        \"pred_pos_rate\": metrics[\"pred_pos_rate\"],\n",
        "                        \"total\": total,\n",
        "                    }\n",
        "                )\n",
        "    df = pd.DataFrame(rows)\n",
        "    df[\"train_pretty\"] = df[\"train_ds\"].map(PRETTY_DATASET_NAMES)\n",
        "    df[\"test_pretty\"] = df[\"test_ds\"].map(PRETTY_DATASET_NAMES)\n",
        "    return df"
      ],
      "metadata": {
        "id": "zhVtiouNY2fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "ziOO26X-mtjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Example: Load data and train base detectors on all_data ----\n",
        "paths = FeaturePaths()  # adjust root if needed\n",
        "all_data = get_dataset(\"all_data\", paths)\n",
        "balanced_all_data = get_dataset(\"balanced_all_data\", paths)"
      ],
      "metadata": {
        "id": "mgYRLsUv4gQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Cell 9 – Example usage / experiment snippets\n",
        "# ============================================\n",
        "\n",
        "dataset_name = \"balanced_all_data\"\n",
        "fc_cfg = FactCheckmateConfig()\n",
        "factcheck_all = train_factcheckmate_for_dataset(balanced_all_data, dataset_name, fc_cfg)\n",
        "\n",
        "lap_cfg = LapEigvalsConfig()\n",
        "lap_all = train_lap_eigvals_for_dataset(balanced_all_data, dataset_name, lap_cfg)\n",
        "\n",
        "icr_cfg = ICRConfig()\n",
        "icr_all = train_icr_probe_for_dataset(balanced_all_data, dataset_name, icr_cfg)\n",
        "\n",
        "llmc_cfg = LLMCheckConfig()\n",
        "llmc_all = train_llmcheck_for_dataset(balanced_all_data, dataset_name, llmc_cfg)\n",
        "\n",
        "ah_cfg = AttnHiddenConfig()\n",
        "ah_all = train_attn_hidden_for_dataset(balanced_all_data, dataset_name, ah_cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xrmssQ6HeFn9",
        "outputId": "544fc698-ccbe-4ccd-fcb0-dd167c5f406a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[balanced_all_data] FactCheckmate X shape: (78788, 2048), y shape: (78788,)\n",
            "Positives: 39394 Negatives: 39394\n",
            "[balanced_all_data][Epoch 01] loss=0.4452 val_auc@0.5=0.9047 val_acc@t*=0.8154 t*=0.50\n",
            "[balanced_all_data][Epoch 02] loss=0.3845 val_auc@0.5=0.9109 val_acc@t*=0.8215 t*=0.55\n",
            "[balanced_all_data][Epoch 03] loss=0.3750 val_auc@0.5=0.9135 val_acc@t*=0.8272 t*=0.50\n",
            "[balanced_all_data][Epoch 04] loss=0.3694 val_auc@0.5=0.9151 val_acc@t*=0.8281 t*=0.45\n",
            "[balanced_all_data][Epoch 05] loss=0.3659 val_auc@0.5=0.9161 val_acc@t*=0.8287 t*=0.50\n",
            "[balanced_all_data][Epoch 06] loss=0.3619 val_auc@0.5=0.9166 val_acc@t*=0.8289 t*=0.45\n",
            "[balanced_all_data][Epoch 07] loss=0.3597 val_auc@0.5=0.9178 val_acc@t*=0.8309 t*=0.45\n",
            "[balanced_all_data][Epoch 08] loss=0.3573 val_auc@0.5=0.9186 val_acc@t*=0.8325 t*=0.45\n",
            "[balanced_all_data][Epoch 09] loss=0.3540 val_auc@0.5=0.9188 val_acc@t*=0.8331 t*=0.45\n",
            "[balanced_all_data][Epoch 10] loss=0.3524 val_auc@0.5=0.9192 val_acc@t*=0.8320 t*=0.45\n",
            "[balanced_all_data][Epoch 11] loss=0.3497 val_auc@0.5=0.9195 val_acc@t*=0.8328 t*=0.40\n",
            "[balanced_all_data][Epoch 12] loss=0.3481 val_auc@0.5=0.9197 val_acc@t*=0.8353 t*=0.45\n",
            "[balanced_all_data][Epoch 13] loss=0.3465 val_auc@0.5=0.9205 val_acc@t*=0.8347 t*=0.45\n",
            "[balanced_all_data][Epoch 14] loss=0.3439 val_auc@0.5=0.9200 val_acc@t*=0.8331 t*=0.45\n",
            "[balanced_all_data][Epoch 15] loss=0.3426 val_auc@0.5=0.9205 val_acc@t*=0.8356 t*=0.45\n",
            "[balanced_all_data][Epoch 16] loss=0.3406 val_auc@0.5=0.9212 val_acc@t*=0.8344 t*=0.45\n",
            "[balanced_all_data][Epoch 17] loss=0.3386 val_auc@0.5=0.9212 val_acc@t*=0.8344 t*=0.45\n",
            "[balanced_all_data][Epoch 18] loss=0.3371 val_auc@0.5=0.9217 val_acc@t*=0.8363 t*=0.40\n",
            "[balanced_all_data][Epoch 19] loss=0.3357 val_auc@0.5=0.9217 val_acc@t*=0.8353 t*=0.50\n",
            "[balanced_all_data][Epoch 20] loss=0.3340 val_auc@0.5=0.9213 val_acc@t*=0.8354 t*=0.45\n",
            "[balanced_all_data][Epoch 21] loss=0.3327 val_auc@0.5=0.9222 val_acc@t*=0.8375 t*=0.45\n",
            "[balanced_all_data][Epoch 22] loss=0.3313 val_auc@0.5=0.9214 val_acc@t*=0.8339 t*=0.50\n",
            "[balanced_all_data][Epoch 23] loss=0.3295 val_auc@0.5=0.9220 val_acc@t*=0.8353 t*=0.45\n",
            "[balanced_all_data][Epoch 24] loss=0.3275 val_auc@0.5=0.9225 val_acc@t*=0.8360 t*=0.50\n",
            "[balanced_all_data][Epoch 25] loss=0.3266 val_auc@0.5=0.9223 val_acc@t*=0.8353 t*=0.50\n",
            "[balanced_all_data][Epoch 26] loss=0.3247 val_auc@0.5=0.9214 val_acc@t*=0.8352 t*=0.45\n",
            "[balanced_all_data][Epoch 27] loss=0.3231 val_auc@0.5=0.9225 val_acc@t*=0.8362 t*=0.50\n",
            "[balanced_all_data][Epoch 28] loss=0.3222 val_auc@0.5=0.9226 val_acc@t*=0.8376 t*=0.45\n",
            "[balanced_all_data][Epoch 29] loss=0.3203 val_auc@0.5=0.9222 val_acc@t*=0.8350 t*=0.50\n",
            "[balanced_all_data][Epoch 30] loss=0.3184 val_auc@0.5=0.9226 val_acc@t*=0.8373 t*=0.50\n",
            "[balanced_all_data][Epoch 31] loss=0.3177 val_auc@0.5=0.9227 val_acc@t*=0.8359 t*=0.45\n",
            "[balanced_all_data][Epoch 32] loss=0.3159 val_auc@0.5=0.9229 val_acc@t*=0.8375 t*=0.50\n",
            "[balanced_all_data][Epoch 33] loss=0.3145 val_auc@0.5=0.9227 val_acc@t*=0.8358 t*=0.45\n",
            "[balanced_all_data][Epoch 34] loss=0.3133 val_auc@0.5=0.9225 val_acc@t*=0.8342 t*=0.55\n",
            "[balanced_all_data][Epoch 35] loss=0.3116 val_auc@0.5=0.9228 val_acc@t*=0.8357 t*=0.45\n",
            "[balanced_all_data][Epoch 36] loss=0.3108 val_auc@0.5=0.9233 val_acc@t*=0.8370 t*=0.45\n",
            "[balanced_all_data][Epoch 37] loss=0.3092 val_auc@0.5=0.9226 val_acc@t*=0.8369 t*=0.50\n",
            "[balanced_all_data][Epoch 38] loss=0.3077 val_auc@0.5=0.9225 val_acc@t*=0.8346 t*=0.55\n",
            "[balanced_all_data][Epoch 39] loss=0.3060 val_auc@0.5=0.9234 val_acc@t*=0.8392 t*=0.50\n",
            "[balanced_all_data][Epoch 40] loss=0.3053 val_auc@0.5=0.9235 val_acc@t*=0.8384 t*=0.45\n",
            "[balanced_all_data][Epoch 41] loss=0.3036 val_auc@0.5=0.9234 val_acc@t*=0.8377 t*=0.55\n",
            "[balanced_all_data][Epoch 42] loss=0.3019 val_auc@0.5=0.9235 val_acc@t*=0.8369 t*=0.55\n",
            "[balanced_all_data][Epoch 43] loss=0.3001 val_auc@0.5=0.9231 val_acc@t*=0.8386 t*=0.45\n",
            "[balanced_all_data][Epoch 44] loss=0.2992 val_auc@0.5=0.9234 val_acc@t*=0.8375 t*=0.50\n",
            "[balanced_all_data][Epoch 45] loss=0.2981 val_auc@0.5=0.9239 val_acc@t*=0.8379 t*=0.50\n",
            "[balanced_all_data][Epoch 46] loss=0.2963 val_auc@0.5=0.9236 val_acc@t*=0.8378 t*=0.50\n",
            "[balanced_all_data][Epoch 47] loss=0.2950 val_auc@0.5=0.9230 val_acc@t*=0.8372 t*=0.50\n",
            "[balanced_all_data][Epoch 48] loss=0.2937 val_auc@0.5=0.9241 val_acc@t*=0.8401 t*=0.50\n",
            "[balanced_all_data][Epoch 49] loss=0.2932 val_auc@0.5=0.9241 val_acc@t*=0.8398 t*=0.45\n",
            "[balanced_all_data][Epoch 50] loss=0.2906 val_auc@0.5=0.9238 val_acc@t*=0.8391 t*=0.45\n",
            "Saved FactCheckmate model to models/balanced_all_data/factcheckmate_model.pth\n",
            "Test metrics (FactCheckmate): {'auroc': np.float64(0.9219102667959828), 'ap': np.float64(0.929622623763707), 'acc': 0.837465098570099, 'f1': 0.8370238398235345, 'precision': 0.8392310309629125, 'recall': 0.8348282281265865, 'threshold': 0.45000000000000007, 'cm': array([[4965,  945],\n",
            "       [ 976, 4933]]), 'tn': 4965, 'fp': 945, 'fn': 976, 'tp': 4933, 'pred_pos_rate': 0.49733479989846857}\n",
            "[balanced_all_data] LapEigvals – Val metrics: {'auroc': np.float64(0.9204226532602369), 'ap': np.float64(0.9262828840915975), 'acc': 0.8328820443391437, 'f1': 0.8305739040919619, 'precision': 0.842205984690327, 'recall': 0.8192587578270435, 'threshold': 0.5, 'cm': array([[5002,  907],\n",
            "       [1068, 4841]]), 'tn': 5002, 'fp': 907, 'fn': 1068, 'tp': 4841, 'pred_pos_rate': 0.4863767134878998}\n",
            "[balanced_all_data] LapEigvals – Test metrics: {'auroc': np.float64(0.9176638693048746), 'ap': np.float64(0.9236767613065482), 'acc': 0.8330654031643963, 'f1': 0.8305419565404105, 'precision': 0.8432159051273108, 'recall': 0.8182433575901168, 'threshold': 0.5, 'cm': array([[5011,  899],\n",
            "       [1074, 4835]]), 'tn': 5011, 'fp': 899, 'fn': 1074, 'tp': 4835, 'pred_pos_rate': 0.48515102800575344}\n",
            "Saved LapEigvals model to models/balanced_all_data/lap_eigvals_model.pth\n",
            "Saved ICR model to models/balanced_all_data/icr_probe_model.pth\n",
            "Saved LLM‑Check model to models/balanced_all_data/llm_check_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Example: meta‑ensemble on all_data ----\n",
        "meta_cfg = MetaConfig()\n",
        "meta_df_all = build_meta_dataframe(dataset_name, paths, base_model_train_source=\"balanced_all_data\", cfg=meta_cfg)\n",
        "\n",
        "meta_state_lr, meta_metrics_lr = train_meta_logreg(meta_df_all, meta_cfg)\n",
        "print(\"Meta logistic metrics:\", meta_metrics_lr)\n",
        "\n",
        "gb_model, gb_info = train_meta_gb(meta_df_all, meta_cfg)\n",
        "print(\"Meta GB metrics:\", gb_info[\"metrics\"])"
      ],
      "metadata": {
        "id": "20aXHW_Xzuf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21e1b9fa-5e94-4f9a-a0e5-11cdbca8d172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta logistic metrics: {'auroc': np.float64(0.9236575265879289), 'ap': np.float64(0.9302076729418869), 'acc': 0.8354486609975885, 'f1': 0.8408909615266613, 'precision': 0.8139700641482538, 'recall': 0.869653509328595, 'threshold': 0.35000000000000003, 'cm': array([[6313, 1566],\n",
            "       [1027, 6852]]), 'tn': 6313, 'fp': 1566, 'fn': 1027, 'tp': 6852, 'pred_pos_rate': 0.5342048483310065}\n",
            "Meta GB metrics: {'auroc': np.float64(0.9244772046475696), 'ap': np.float64(0.9318459691824587), 'acc': 0.8395100901129585, 'f1': 0.8400682982356289, 'precision': 0.8371565414671036, 'recall': 0.8430003807589795, 'threshold': 0.45000000000000007, 'cm': array([[6587, 1292],\n",
            "       [1237, 6642]]), 'tn': 6587, 'fp': 1292, 'fn': 1237, 'tp': 6642, 'pred_pos_rate': 0.503490290646021}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Example: cross‑dataset base experiments ----\n",
        "cross_results = run_cross_dataset_base_experiments(paths)\n",
        "df_cross = cross_results_to_dataframe(cross_results)\n",
        "df_cross.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 900
        },
        "collapsed": true,
        "id": "1GGLqRwl-kQT",
        "outputId": "74a040f8-ecbb-439b-c17e-ae1def3370f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating factcheckmate: train=defan, test=mmlu\n",
            "Evaluating factcheckmate: train=defan, test=halueval_qa\n",
            "Evaluating factcheckmate: train=defan, test=halueval_summarization\n",
            "Evaluating factcheckmate: train=defan, test=halueval_dialogue\n",
            "Evaluating factcheckmate: train=defan, test=psiloqa\n",
            "Evaluating factcheckmate: train=mmlu, test=defan\n",
            "Evaluating factcheckmate: train=mmlu, test=halueval_qa\n",
            "Evaluating factcheckmate: train=mmlu, test=halueval_summarization\n",
            "Evaluating factcheckmate: train=mmlu, test=halueval_dialogue\n",
            "Evaluating factcheckmate: train=mmlu, test=psiloqa\n",
            "Evaluating factcheckmate: train=halueval_qa, test=defan\n",
            "Evaluating factcheckmate: train=halueval_qa, test=mmlu\n",
            "Evaluating factcheckmate: train=halueval_qa, test=halueval_summarization\n",
            "Evaluating factcheckmate: train=halueval_qa, test=halueval_dialogue\n",
            "Evaluating factcheckmate: train=halueval_qa, test=psiloqa\n",
            "Evaluating factcheckmate: train=halueval_summarization, test=defan\n",
            "Evaluating factcheckmate: train=halueval_summarization, test=mmlu\n",
            "Evaluating factcheckmate: train=halueval_summarization, test=halueval_qa\n",
            "Evaluating factcheckmate: train=halueval_summarization, test=halueval_dialogue\n",
            "Evaluating factcheckmate: train=halueval_summarization, test=psiloqa\n",
            "Evaluating factcheckmate: train=halueval_dialogue, test=defan\n",
            "Evaluating factcheckmate: train=halueval_dialogue, test=mmlu\n",
            "Evaluating factcheckmate: train=halueval_dialogue, test=halueval_qa\n",
            "Evaluating factcheckmate: train=halueval_dialogue, test=halueval_summarization\n",
            "Evaluating factcheckmate: train=halueval_dialogue, test=psiloqa\n",
            "Evaluating factcheckmate: train=psiloqa, test=defan\n",
            "Evaluating factcheckmate: train=psiloqa, test=mmlu\n",
            "Evaluating factcheckmate: train=psiloqa, test=halueval_qa\n",
            "Evaluating factcheckmate: train=psiloqa, test=halueval_summarization\n",
            "Evaluating factcheckmate: train=psiloqa, test=halueval_dialogue\n",
            "Evaluating lap_eigvals: train=defan, test=mmlu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'models/defan/lap_eigvals_model.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-88888891.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ---- Example: cross‑dataset base experiments ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcross_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_cross_dataset_base_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_cross\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_results_to_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_cross\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-415099321.py\u001b[0m in \u001b[0;36mrun_cross_dataset_base_experiments\u001b[0;34m(paths, datasets, methods)\u001b[0m\n\u001b[1;32m     56\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Evaluating {method}: train={train_ds}, test={test_ds}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_base_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-415099321.py\u001b[0m in \u001b[0;36mevaluate_base_method\u001b[0;34m(method, train_dataset, test_dataset, paths)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mevaluate_factcheckmate_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"lap_eigvals\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_lap_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mevaluate_lap_eigvals_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"icr_probe\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3506736488.py\u001b[0m in \u001b[0;36mload_lap_model\u001b[0;34m(dataset_name)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_lap_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_lap_model_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/defan/lap_eigvals_model.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization/Plotting Helpers"
      ],
      "metadata": {
        "id": "bSuCxxjQPNGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Cell 10 – Plotting helpers\n",
        "# ============================================\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
        "\n",
        "\n",
        "# ---------------- Basic helpers ----------------\n",
        "\n",
        "def plot_confusion_matrix_from_metrics(\n",
        "    metrics: dict,\n",
        "    class_names=(\"Non-hallucination\", \"Hallucination\"),\n",
        "    normalize: bool = False,\n",
        "    title: str = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot a 2x2 confusion matrix from a `metrics` dict created by compute_binary_metrics().\n",
        "    \"\"\"\n",
        "    cm = np.array(metrics[\"cm\"], dtype=float)\n",
        "\n",
        "    if normalize:\n",
        "        row_sums = cm.sum(axis=1, keepdims=True)\n",
        "        row_sums[row_sums == 0] = 1.0\n",
        "        cm_display = cm / row_sums\n",
        "    else:\n",
        "        cm_display = cm\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(4, 4))\n",
        "    im = ax.imshow(cm_display, aspect=\"equal\")\n",
        "\n",
        "    ax.set_xticks(np.arange(2))\n",
        "    ax.set_yticks(np.arange(2))\n",
        "    ax.set_xticklabels(class_names)\n",
        "    ax.set_yticklabels(class_names)\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"True\")\n",
        "\n",
        "    if title is not None:\n",
        "        ax.set_title(title)\n",
        "\n",
        "    # annotate\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            if normalize:\n",
        "                text = f\"{cm_display[i, j]:.2f}\"\n",
        "            else:\n",
        "                text = f\"{int(cm[i, j])}\"\n",
        "            ax.text(j, i, text, ha=\"center\", va=\"center\")\n",
        "\n",
        "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_roc_pr_curves(\n",
        "    y_true,\n",
        "    y_prob,\n",
        "    title_prefix: str = \"Model\",\n",
        "    show_baselines: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot ROC and Precision–Recall curves given y_true and y_prob.\n",
        "    (Use if you keep around raw logits/probabilities yourself.)\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "\n",
        "    # ROC\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # PR\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
        "    pr_auc = auc(recall, precision)\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 4))\n",
        "\n",
        "    # ROC\n",
        "    ax1 = fig.add_subplot(1, 2, 1)\n",
        "    ax1.plot(fpr, tpr)\n",
        "    if show_baselines:\n",
        "        ax1.plot([0, 1], [0, 1], linestyle=\"--\")\n",
        "    ax1.set_xlabel(\"False Positive Rate\")\n",
        "    ax1.set_ylabel(\"True Positive Rate\")\n",
        "    ax1.set_title(f\"{title_prefix} ROC (AUC={roc_auc:.3f})\")\n",
        "\n",
        "    # PR\n",
        "    ax2 = fig.add_subplot(1, 2, 2)\n",
        "    ax2.plot(recall, precision)\n",
        "    if show_baselines:\n",
        "        base_rate = y_true.mean()\n",
        "        ax2.hlines(base_rate, 0, 1, linestyles=\"--\")\n",
        "    ax2.set_xlabel(\"Recall\")\n",
        "    ax2.set_ylabel(\"Precision\")\n",
        "    ax2.set_title(f\"{title_prefix} PR (AUC={pr_auc:.3f})\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------- Cross-dataset plots ----------------\n",
        "\n",
        "def plot_cross_dataset_heatmap(\n",
        "    df_cross: pd.DataFrame,\n",
        "    metric: str = \"f1\",\n",
        "    method: str = \"factcheckmate\",\n",
        "    use_pretty_names: bool = True,\n",
        "    annotate: bool = True,\n",
        "    figsize=(7, 5),\n",
        "):\n",
        "    \"\"\"\n",
        "    Heatmap of cross-dataset performance for a single method.\n",
        "\n",
        "    df_cross: output of cross_results_to_dataframe(cross_results)\n",
        "    metric: one of ['f1', 'acc', 'auroc', 'ap', 'precision', 'recall', 'pred_pos_rate']\n",
        "    method: one of BASE_METHODS\n",
        "    \"\"\"\n",
        "    subset = df_cross[df_cross[\"method\"] == method].copy()\n",
        "    if subset.empty:\n",
        "        raise ValueError(f\"No rows for method={method} in df_cross\")\n",
        "\n",
        "    if use_pretty_names and \"train_pretty\" in subset.columns:\n",
        "        row_key = \"train_pretty\"\n",
        "        col_key = \"test_pretty\"\n",
        "    else:\n",
        "        row_key = \"train_ds\"\n",
        "        col_key = \"test_ds\"\n",
        "\n",
        "    pivot = subset.pivot(index=row_key, columns=col_key, values=metric)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    im = ax.imshow(pivot.values, aspect=\"auto\")\n",
        "\n",
        "    ax.set_xticks(np.arange(pivot.shape[1]))\n",
        "    ax.set_yticks(np.arange(pivot.shape[0]))\n",
        "    ax.set_xticklabels(pivot.columns, rotation=45, ha=\"right\")\n",
        "    ax.set_yticklabels(pivot.index)\n",
        "    ax.set_xlabel(\"Test dataset\")\n",
        "    ax.set_ylabel(\"Train dataset\")\n",
        "    ax.set_title(f\"{method} – {metric.upper()} cross-dataset\")\n",
        "\n",
        "    if annotate:\n",
        "        for i in range(pivot.shape[0]):\n",
        "            for j in range(pivot.shape[1]):\n",
        "                val = pivot.values[i, j]\n",
        "                if np.isnan(val):\n",
        "                    text = \"\"\n",
        "                else:\n",
        "                    text = f\"{val:.2f}\"\n",
        "                ax.text(j, i, text, ha=\"center\", va=\"center\")\n",
        "\n",
        "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_cross_dataset_method_comparison(\n",
        "    df_cross: pd.DataFrame,\n",
        "    metric: str = \"f1\",\n",
        "    train_ds: str = None,\n",
        "    test_ds: str = None,\n",
        "    use_pretty_names: bool = True,\n",
        "    figsize=(7, 4),\n",
        "):\n",
        "    \"\"\"\n",
        "    Bar plot comparing methods for a fixed (train_ds, test_ds) pair.\n",
        "\n",
        "    If train_ds or test_ds is None, uses all rows and averages across them.\n",
        "    \"\"\"\n",
        "    df = df_cross.copy()\n",
        "\n",
        "    if train_ds is not None:\n",
        "        df = df[df[\"train_ds\"] == train_ds]\n",
        "    if test_ds is not None:\n",
        "        df = df[df[\"test_ds\"] == test_ds]\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(\"No rows left after filtering by train_ds/test_ds.\")\n",
        "\n",
        "    grouped = df.groupby(\"method\")[metric].mean().reset_index()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    x = np.arange(len(grouped))\n",
        "    ax.bar(x, grouped[metric])\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(grouped[\"method\"], rotation=45, ha=\"right\")\n",
        "    ax.set_ylabel(metric.upper())\n",
        "\n",
        "    if train_ds is not None and test_ds is not None:\n",
        "        if use_pretty_names and train_ds in PRETTY_DATASET_NAMES and test_ds in PRETTY_DATASET_NAMES:\n",
        "            train_label = PRETTY_DATASET_NAMES[train_ds]\n",
        "            test_label = PRETTY_DATASET_NAMES[test_ds]\n",
        "        else:\n",
        "            train_label = train_ds\n",
        "            test_label = test_ds\n",
        "        title = f\"{metric.upper()} – train={train_label}, test={test_label}\"\n",
        "    else:\n",
        "        title = f\"{metric.upper()} – mean across available pairs\"\n",
        "\n",
        "    ax.set_title(title)\n",
        "    ax.set_ylim(0, 1.0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------- Base vs meta ensemble ----------------\n",
        "\n",
        "def plot_base_vs_meta_bar(\n",
        "    base_metrics: dict,\n",
        "    meta_metrics: dict,\n",
        "    metric: str = \"f1\",\n",
        "    title: str = None,\n",
        "    figsize=(7, 4),\n",
        "):\n",
        "    \"\"\"\n",
        "    Compare base methods against a meta-ensemble (e.g., logistic meta model).\n",
        "\n",
        "    base_metrics: dict method_name -> metrics_dict (from compute_binary_metrics or training funcs)\n",
        "    meta_metrics: metrics_dict for the meta model\n",
        "    metric: which metric to plot (e.g. 'f1', 'auroc', 'acc')\n",
        "    \"\"\"\n",
        "    names = list(base_metrics.keys()) + [\"meta\"]\n",
        "    vals = [base_metrics[m][metric] for m in base_metrics] + [meta_metrics[metric]]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    x = np.arange(len(names))\n",
        "    ax.bar(x, vals)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(names, rotation=45, ha=\"right\")\n",
        "    ax.set_ylabel(metric.upper())\n",
        "    ax.set_ylim(0, 1.0)\n",
        "\n",
        "    if title is None:\n",
        "        title = f\"Base vs Meta – {metric.upper()}\"\n",
        "    ax.set_title(title)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "meCzr2egNnv5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}